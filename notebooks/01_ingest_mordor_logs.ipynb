{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6300430c-f257-49df-a476-dc125524deed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project Cyber-Trace: Bronze Layer Ingestion\n",
    "**Author:** Jakub Milczarczyk  \n",
    "**Description:** This notebook handles the ingestion of raw security logs (OTRF/Mordor dataset) from **Azure Data Lake Gen2 (ADLS)** into the Bronze Layer. It utilizes **Databricks Auto Loader** for scalable, incremental processing.\n",
    "\n",
    "## 1. Environment Setup & Security\n",
    "This section configures the connection between Databricks and Azure Storage.\n",
    "\n",
    "### Architecture Note: Secure Authentication\n",
    "**Standard Industry Practice:** Hardcoding credentials (access keys) in notebooks is a critical security risk.\n",
    "**Selected Solution:** I utilize **Azure Key Vault** integrated with **Databricks Secret Scopes**.\n",
    "* The Access Key is stored securely in Azure Key Vault.\n",
    "* Databricks retrieves it at runtime using `dbutils.secrets.get()`.\n",
    "* This ensures no sensitive data is exposed in the source code or version control systems (Git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9f811d-6582-4feb-8922-2e75bbacd765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: CONFIGURATION & AUTHENTICATION\n",
    "# Environment setup - Safe to run in any environment\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Resource Definitions\n",
    "storage_account_name = \"cybertracebronze\"\n",
    "container_name       = \"bronze\"\n",
    "scope_name           = \"cybertrace-secrets\"\n",
    "key_name             = \"storage-access-key\"\n",
    "\n",
    "# 2. Secure Authentication\n",
    "try:\n",
    "    sas_key = dbutils.secrets.get(scope=scope_name, key=key_name)\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "        sas_key\n",
    "    )\n",
    "    print(\"SYSTEM: Authentication configured successfully.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"CRITICAL ERROR: Failed to retrieve secrets. Details: {e}\")\n",
    "\n",
    "# 3. Path Definitions\n",
    "base_path       = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "raw_logs_path   = f\"{base_path}/raw_logs\"\n",
    "schema_path     = f\"{base_path}/schemas/ingestion_schema\"\n",
    "checkpoint_path = f\"{base_path}/checkpoints/raw_logs_ingest\"\n",
    "\n",
    "print(f\"CONFIG: Paths set. Ingestion target: {raw_logs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef10b3c1-bffd-4cb4-a0e3-18e4c4a0937f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Development State Reset (⚠️ DEV ONLY)\n",
    "**Objective:** To simulate a \"fresh start\" of the ingestion pipeline during the prototyping phase.\n",
    "\n",
    "**Context:** Spark Structured Streaming maintains the state of processed files in a **Checkpoint** location. To re-process the same dataset or test configuration changes, we must clear this metadata.\n",
    "\n",
    "> **Warning:** This step deletes the checkpoint history. In a **Production** environment, this cell would be disabled or removed to prevent data duplication and loss of processing history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ebe3e37-dad6-41cb-a79b-b4453d74cc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: ENVIRONMENT RESET (⚠️ DEV ONLY ⚠️)\n",
    "# Executes a cleanup to restart streaming from scratch.\n",
    "# DO NOT RUN THIS IN PRODUCTION or you will lose processing history!\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"MAINTENANCE: Cleaning up checkpoints and schemas for a fresh start...\")\n",
    "\n",
    "# Remove checkpoints (resets stream offset to zero)\n",
    "dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "\n",
    "# Optional: Remove inferred schema if you want to re-learn column types\n",
    "# dbutils.fs.rm(schema_path, recurse=True) \n",
    "\n",
    "print(\"MAINTENANCE: Environment clean. Ready for fresh ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4336905c-ea88-42fe-88a0-c84c8eb1dfe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Ingestion Pipeline (Auto Loader)\n",
    "**Objective:** ingest raw JSON logs efficiently and robustly.\n",
    "\n",
    "**Technology Stack:**\n",
    "* **Databricks Auto Loader (`cloudFiles`):** An optimized file source that detects new files as they arrive in ADLS without listing the entire directory (solving the \"S3/ADLS listing\" performance bottleneck).\n",
    "* **Schema Evolution:** The pipeline is configured to automatically detect and adapt to changes in the log structure (e.g., new fields in JSON events) using `schema_path`.\n",
    "* **Checkpointing:** Ensures fault tolerance. If the cluster crashes, the stream resumes exactly where it left off.\n",
    "\n",
    "**Output:**\n",
    "For this prototype, the stream writes to an **in-memory table** (`memory` format) for immediate validation. In the next stage (Silver Layer), this will be replaced with a **Delta Lake** sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886b89d4-b714-41d9-a444-d6b5f0e1d56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: DATA INGESTION PIPELINE\n",
    "# Core logic: Read Stream -> Write Memory -> Verify\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define the Stream (Auto Loader)\n",
    "print(f\"ACTION: Initializing Auto Loader from {raw_logs_path}...\")\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .load(raw_logs_path)\n",
    ")\n",
    "\n",
    "# 2. Run the Stream (In-Memory for testing)\n",
    "temp_table_name = \"raw_logs_view\"\n",
    "\n",
    "# We use the checkpoint defined in Cell 1. \n",
    "# If Cell 2 was run, it starts from scratch. If not, it resumes.\n",
    "query = (df_stream.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(temp_table_name)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"SYSTEM: Stream is running. Waiting 5 seconds for data availability...\")\n",
    "import time\n",
    "time.sleep(5) \n",
    "\n",
    "# 3. Data Verification\n",
    "print(f\"REPORT: Latest logs preview (Top 10):\")\n",
    "df_view = spark.sql(f\"SELECT * FROM {temp_table_name} LIMIT 10\")\n",
    "display(df_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8236b881-fd06-4739-9b83-19169ea14a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_mordor_logs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
